<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>MedAI Fluency Studio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/canvas-confetti@1.6.0/dist/confetti.browser.min.js"></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;600;700&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
  <style>
    html { scroll-behavior: smooth; }
    body { font-family: 'Roboto', sans-serif; background-color: #f8fafc; color: #1e293b; }
    h1 { font-family: 'Playfair Display', serif; font-weight: 700; }
    .question-text { font-weight: 600; }
    .answer-text { font-weight: 400; }
    .option { @apply flex items-center gap-4 p-4 rounded-lg cursor-pointer hover:bg-indigo-50 transition; }
    .option input { @apply w-5 h-5 text-indigo-600; }
  </style>
</head>
<body class="min-h-screen py-12">
  <div class="max-w-4xl mx-auto px-6">
    <header class="text-center mb-16">
      <h1 class="text-3xl md:text-4xl font-bold text-slate-900 mb-4">MedAI Fluency Studio</h1>
      <p class="text-lg text-slate-600">20 personalized questions • 5-minute assessment • Instant results</p>
    </header>

    <form id="quiz" class="space-y-10 mb-24"></form>

    <section id="results" class="hidden">
      <div class="bg-white rounded-3xl shadow-2xl p-10 md:p-14 text-center">
        <h2 class="text-3xl md:text-4xl font-bold text-slate-900 mb-10">Your MedAI Fluency Profile</h2>
        <div class="max-w-md mx-auto">
          <canvas id="radar"></canvas>
        </div>
        <p class="text-5xl font-bold text-indigo-700 mt-10" id="score"></p>
        <p class="text-xl text-slate-700 mt-4" id="message"></p>
        <div id="focus-areas" class="mt-10 text-base text-slate-700"></div>
        <div class="mt-12 flex flex-col sm:flex-row gap-6 justify-center">
          <a href="micromodules.html" class="px-10 py-5 bg-indigo-600 text-white font-semibold text-base rounded-full hover:bg-indigo-700 transition shadow-lg">
            Watch Recommended Videos
          </a>
          <button onclick="location.reload()" class="px-10 py-5 bg-white border-2 border-indigo-600 text-indigo-700 font-semibold text-base rounded-full hover:bg-indigo-50 transition">
            Take Quiz Again
          </button>
        </div>
        <details class="mt-12 max-w-3xl mx-auto text-left">
          <summary class="text-xl font-semibold text-slate-700 cursor-pointer hover:text-indigo-700 py-4">
            Show questions I got wrong
          </summary>
          <div id="review" class="mt-6 space-y-6"></div>
        </details>
      </div>
    </section>
  </div>

  <script>
    const questionBank = [
      // CLUSTER 1: Prompt Mastery & Conversation Patterns (20 questions)
      {q:"Chain-of-thought prompting improves reasoning by:", a:"Letting the model generate intermediate steps", d:["Making the model think faster","Reducing the number of tokens used","Automatically adding safety filters"], e:"CoT gives the model space to reason step-by-step (Module 09).", c:1},
      {q:"Higher temperature setting produces:", a:"More creative / diverse output", d:["More accurate medical advice","Shorter responses","Lower cost per token"], e:"Use low temperature for reliable clinical answers (Module 01).", c:1},
      {q:"Few-shot prompting means:", a:"Including examples in the prompt", d:["Using zero examples","Fine-tuning the model","Adding images to the prompt"], e:"Few-shot guides the model without retraining (Module 01).", c:1},
      {q:"The system prompt is used to:", a:"Define the AI's role and behavior rules", d:["Set the output length only","Define the token limit","Set the temperature value"], e:"System prompt sets personality and safety rules (Module 01).", c:1},
      {q:"Style transfer in prompts refers to:", a:"Making output match a specific tone or format", d:["Changing the model architecture","Transferring data between models","Styling the user interface"], e:"Used for patient education in simple language (Module 01).", c:1},
      {q:"Zero-shot prompting means:", a:"No examples given", d:["One example given","Multiple examples given","Fine-tuning required"], e:"Relies only on pre-training (Module 01).", c:1},
      {q:"Best practice for medical prompts is to:", a:"Be specific, provide context, ask for reasoning", d:["Keep it vague to encourage creativity","Use high temperature for accuracy","Avoid mentioning the patient's condition"], e:"Clear prompts lead to safer answers (Module 01).", c:1},
      {q:"Why do models sometimes refuse tasks?", a:"Because of safety filters in post-training", d:["They are out of tokens","The prompt is too long","The temperature is too low"], e:"Safety filters from RLHF (Module 14).", c:1},
      {q:"Temperature = 0 means:", a:"Fully deterministic output", d:["Maximum creativity","Random sampling","Error mode"], e:"Always the most likely token (Module 01).", c:1},
      {q:"Sampling methods like top-p affect:", a:"How creative the output is", d:["Token count only","Model size","Training time"], e:"Controls diversity of responses (Module 01).", c:1},
      {q:"For clinical questions you want:", a:"Low temperature so answers are stable", d:["High temperature for brainstorming","No temperature setting","Random sampling"], e:"Reproducible answers are safer (Module 01).", c:1},
      {q:"CoT is especially useful for:", a:"Complex reasoning or long chain explanations", d:["Simple fact lookup","Image generation","Code execution"], e:"Step-by-step differentials reduce errors (Module 09).", c:1},
      {q:"System prompt is most important for:", a:"Consistent behavior across conversations", d:["Speeding up inference","Reducing hallucinations","Adding more context tokens"], e:"Sets the rules for the entire session (Module 01).", c:1},
      {q:"Few-shot is better than zero-shot when:", a:"You can provide relevant examples", d:["The prompt is very short","Temperature is high","Model is small"], e:"Examples guide the model (Module 01).", c:1},
      {q:"Prompt engineering is like:", a:"Writing a focused question for radiology", d:["Building a model from scratch","Training on new data","Fine-tuning LoRA adapters"], e:"Clear consult request = clear answer (Module 01).", c:1},
      {q:"The best prompts include:", a:"Brief clinical history, key concern, what you want clarified", d:["Just 'please comment'","Vague questions","High token count only"], e:"Structure leads to better output (Module 01).", c:1},
      {q:"Top-k sampling chooses from:", a:"The top k most likely tokens", d:["All tokens equally","Random tokens","Only safe tokens"], e:"Controls diversity (Module 01).", c:1},
      {q:"Nucleus sampling (top-p) is better than top-k because:", a:"It adapts to the probability distribution", d:["It uses fewer tokens","It is faster","It reduces cost"], e:"Dynamic selection (Module 01).", c:
