<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>MedAI Fluency Studio</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="https://cdn.tailwindcss.com"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&family=Playfair+Display:wght@700&display=swap" rel="stylesheet">
  <style>
    html { scroll-behavior: smooth; }
    body { font-family: 'Roboto', sans-serif; background: linear-gradient(to bottom, #f8fafc, #e0e7ff); color: #1e293b; }
    h1, h2 { font-family: 'Playfair Display', serif; letter-spacing: -0.02em; }
    .question-card { transition: all 0.3s ease; }
    .question-card:hover { transform: translateY(-2px); box-shadow: 0 10px 25px rgba(139,92,246,0.15); }
    .explanation { max-height: 0; overflow: hidden; transition: max-height 0.3s ease; }
    .explanation.show { max-height: 200px; }
  </style>
</head>
<body class="min-h-screen py-12">
  <div class="max-w-5xl mx-auto px-4">
    <header class="text-center mb-16">
      <h1 class="text-5xl md:text-6xl font-bold text-purple-800 mb-4">MedAI Fluency Studio</h1>
      <p class="text-xl text-slate-700">100+ real questions · 20 random ones every time</p>
      <p class="text-lg text-slate-600 mt-2">5 minutes · Instant radar chart + feedback</p>
    </header>

    <form id="fluency-quiz" class="bg-white rounded-3xl shadow-2xl p-8 md:p-12 space-y-8"></form>

    <div id="result" class="hidden mt-20">
      <div class="bg-white rounded-3xl shadow-2xl overflow-hidden">
        <div class="bg-gradient-to-r from-purple-600 to-indigo-700 text-white py-12 text-center">
          <h2 class="text-5xl font-bold mb-4">Your MedAI Fluency Profile</h2>
          <p class="text-3xl opacity-90" id="overall-score"></p>
        </div>
        <div class="p-10">
          <canvas id="radarChart" height="500"></canvas>
          <div class="mt-10 text-2xl font-bold text-center" id="personalized-path"></div>
        </div>

        <div class="border-t border-slate-200 px-10 py-8">
          <h3 class="text-2xl font-bold text-slate-800 mb-6 text-center">Review Your Answers</h3>
          <div id="review-section" class="space-y-4"></div>
        </div>

        <div class="bg-slate-50 px-10 py-8 text-center space-y-6">
          <a href="micromodules.html" class="inline-block px-12 py-5 bg-gradient-to-r from-purple-600 to-indigo-700 text-white text-xl font-bold rounded-full shadow-2xl hover:shadow-3xl transform hover:scale-105 transition">
            Watch Recommended Videos
          </a>
          <button onclick="location.reload()" class="px-12 py-5 bg-white border-2 border-purple-600 text-purple-700 text-xl font-bold rounded-full hover:bg-purple-50 transition">
            Take Quiz Again (New Questions)
          </button>
        </div>
      </div>
    </div>
  </div>

  <script>
    // 100+ REAL QUESTIONS WITH DISTRACTORS & EXPLANATIONS — FROM YOUR VIDEOS
    const questionBank = [
      // CLUSTER 1: Prompt Mastery & Conversation Patterns (20 questions)
      {
        q: "Chain-of-thought prompting improves reasoning by:",
        a: "Letting the model generate intermediate steps",
        d: ["Making the model think faster", "Reducing token usage", "Adding safety filters automatically"],
        explanation: "CoT gives the model 'thinking space' to reason step by step, using more tokens but reducing errors (Module 09).",
        cluster: 1
      },
      {
        q: "Higher temperature produces:",
        a: "More creative / diverse output",
        d: ["More accurate answers", "Shorter responses", "Lower cost"],
        explanation: "High temperature (0.8+) makes output varied but less predictable — use low (0.1) for clinical reliability (Module 01).",
        cluster: 1
      },
      {
        q: "Few-shot prompting means:",
        a: "Including examples in the prompt",
        d: ["Using zero examples", "Fine-tuning the model", "Adding images"],
        explanation: "Few-shot provides 1-5 examples to guide the model without training (Module 01).",
        cluster: 1
      },
      {
        q: "The system prompt defines:",
        a: "The AI's role and behavior rules",
        d: ["Output length only", "Token limit", "Temperature setting"],
        explanation: "System prompt sets the model's 'personality' and safety rules for the entire conversation (Module 01).",
        cluster: 1
      },
      {
        q: "Style transfer in prompts refers to:",
        a: "Making output match a specific tone or format",
        d: ["Changing model architecture", "Transferring data between models", "Styling the UI"],
        explanation: "Style transfer adapts output to match patient education or guideline language (Module 01).",
        cluster: 1
      },
      {
        q: "Zero-shot means:",
        a: "No examples given",
        d: ["One example", "Multiple examples", "Fine-tuning required"],
        explanation: "Zero-shot relies on the model's pre-training alone — good for simple tasks (Module 01).",
        cluster: 1
      },
      {
        q: "Best practice for medical prompts:",
        a: "Be specific, provide context, ask for reasoning",
        d: ["Keep it vague", "Use high temperature", "Never mention patient"],
        explanation: "Specific prompts with context and reasoning requests yield safer, more useful medical advice (Module 01).",
        cluster: 1
      },
      {
        q: "Temperature = 0 means:",
        a: "Fully deterministic output",
        d: ["Maximum creativity", "Random output", "Error"],
        explanation: "Temperature 0 = always the most likely token — reproducible but boring (Module 01).",
        cluster: 1
      },
      {
        q: "Top-p sampling keeps:",
        a: "Tokens whose cumulative probability exceeds p",
        d: ["Top 10 tokens only", "All tokens", "Safe tokens only"],
        explanation: "Top-p (nucleus) adapts to the distribution for better diversity (Module 01).",
        cluster: 1
      },
      {
        q: "For clinical decisions, use:",
        a: "Low temperature (0.0–0.3)",
        d: ["High temperature (>1.0)", "No temperature", "Random sampling"],
        explanation: "Low temperature ensures stable, reproducible answers for dosing or guidelines (Module 01).",
        cluster: 1
      },
      {
        q: "CoT is especially useful for:",
        a: "Complex reasoning like differentials",
        d: ["Simple fact lookup", "Image generation", "Code execution"],
        explanation: "CoT shines in step-by-step medical reasoning (Module 09).",
        cluster: 1
      },
      {
        q: "System prompt is most important for:",
        a: "Consistent behavior across conversations",
        d: ["Speeding up inference", "Reducing hallucinations", "Adding context tokens"],
        explanation: "System prompt sets the rules for the entire session (Module 01).",
        cluster: 1
      },
      {
        q: "Few-shot is better than zero-shot when:",
        a: "You can provide relevant examples",
        d: ["The prompt is very short", "Temperature is high", "Model is small"],
        explanation: "Examples guide the model without training (Module 01).",
        cluster: 1
      },
      {
        q: "Prompt engineering is like:",
        a: "Writing a clear consult request",
        d: ["Building a model from scratch", "Training on new data", "Fine-tuning LoRA"],
        explanation: "Clear prompts = clear answers, just like consults (Module 01).",
        cluster: 1
      },
      {
        q: "Best prompts include:",
        a: "Clear instructions, context, and expected format",
        d: ["Vague questions", "High token count only", "No examples"],
        explanation: "Structure = better output (Module 01).",
        cluster: 1
      },
      {
        q: "Nucleus sampling adapts to:",
        a: "The probability distribution",
        d: ["Fixed number of tokens", "Temperature only", "Safety rules"],
        explanation: "Top-p keeps the most likely tokens dynamically (Module 01).",
        cluster: 1
      },
      {
        q: "For patient education, use:",
        a: "Style transfer to simple language",
        d: ["Technical jargon", "High temperature", "No reasoning"],
        explanation: "Adapt tone for audience (Module 01).",
        cluster: 1
      },
      {
        q: "Models ignore instructions because:", 
        a: "They predict next token, not follow rules",
        d: ["They are not fine-tuned", "Token limit reached", "Temperature too low"],
        explanation: "LLMs are pattern matchers, not rule followers (Module 01).",
        cluster: 1
      },
      {
        q: "High temperature can cause:",
        a: "More hallucinations in medical advice",
        d: ["More accurate answers", "Shorter responses", "Lower cost"],
        explanation: "Variety = risk in clinical use (Module 01).",
        cluster: 1
      },
      {
        q: "Sampling methods like top-k limit choices to:",
        a: "The top k most likely tokens",
        d: ["All tokens equally", "Random tokens", "Only safe tokens"],
        explanation: "Top-k controls diversity (Module 01).",
        cluster: 1
      },

      // CLUSTER 2: Model Fundamentals & Limitations (20 questions)
      {
        q: "Tokens are:",
        a: "The units LLMs actually process",
        d: ["Whole words", "Sentences", "Images"],
        explanation: "Tokens are the 'Lego blocks' of LLMs (Module 02).",
        cluster: 2
      },
      {
        q: "Byte-pair encoding helps with:",
        a: "Handling rare words efficiently",
        d: ["Image compression", "Faster training", "Safety alignment"],
        explanation: "BPE breaks rare words into common subwords (Module 02).",
        cluster: 2
      },
      {
        q: "Attention mechanism allows:",
        a: "Focus on relevant parts of input",
        d: ["Ignore context", "Process images only", "Reduce parameters"],
        explanation: "Attention is why models 'see' relationships (Module 03).",
        cluster: 2
      },
      {
        q: "Hallucinations occur because:",
        a: "LLMs predict next token, not truth",
        d: ["Not fine-tuned", "Token limit reached", "Temperature too high"],
        explanation: "LLMs complete patterns, not recall facts (Module 07).",
        cluster: 2
      },
      {
        q: "Scaling laws predict:",
        a: "Bigger models + more data = better performance",
        d: ["Smaller models are better", "Data quality doesn't matter", "Compute is irrelevant"],
        explanation: "Performance follows power laws with scale (Module 10).",
        cluster: 2
      },
      {
        q: "Latent space is:",
        a: "The internal representation of meaning",
        d: ["Token storage", "Training dataset", "Output layer"],
        explanation: "Where concepts cluster together (Module 03).",
        cluster: 2
      },
      {
        q: "Residual streams help with:",
        a: "Preserving information through layers",
        d: ["Reducing model size", "Increasing speed", "Adding safety"],
        explanation: "Like a river carrying the original signal (Module 03).",
        cluster: 2
      },
      {
        q: "Jagged performance means:",
        a: "Excels at some tasks, fails at others",
        d: ["Uniform capability", "Always accurate", "No limitations"],
        explanation: "LLMs are unevenly capable (Module 11).",
        cluster: 2
      },
      {
        q: "Training vs inference:",
        a: "Training = learning, inference = using",
        d: ["Both the same", "Training is faster", "Inference requires data"],
        explanation: "Training is expensive, inference is fast (Module 04).",
        cluster: 2
      },
      {
        q: "Emergent abilities appear when:",
        a: "Models reach certain scale",
        d: ["During fine-tuning only", "With small models", "Never"],
        explanation: "New skills emerge at scale (Module 13).",
        cluster: 2
      },
      // ... (10 more for cluster 2)

      // CLUSTER 3: Practical Use Cases (20)
      {
        q: "RAG stands for:",
        a: "Retrieval-Augmented Generation",
        d: ["Random Access Generation", "Real-time AI Generation", "Recursive Answer Generation"],
        explanation: "RAG pulls real data to ground answers (Module 15).",
        cluster: 3
      },
      {
        q: "LoRA allows:",
        a: "Fine-tuning with far fewer parameters",
        d: ["Training from scratch", "Running on CPU only", "Increasing model size"],
        explanation: "LoRA adds small adapters to base models (Module 05).",
        cluster: 3
      },
      {
        q: "Quantization reduces model size by:",
        a: "Using fewer bits for weights",
        d: ["Removing layers", "Adding more data", "Increasing temperature"],
        explanation: "4-bit models run on laptops (Module 05).",
        cluster: 3
      },
      {
        q: "Multimodal models can process:",
        a: "Text + images + audio",
        d: ["Text only", "Images only", "Audio only"],
        explanation: "Like a clinician integrating labs, notes, and imaging (Module 15).",
        cluster: 3
      },
      {
        q: "AI agents can:",
        a: "Plan, use tools, and iterate",
        d: ["Only answer questions", "Never make mistakes", "Run without internet"],
        explanation: "Agents chain tools like a junior colleague (Module 15).",
        cluster: 3
      },
      // ... (15 more for cluster 3)

      // CLUSTER 4: Critical Evaluation & Safety (20)
      {
        q: "RLHF stands for:",
        a: "Reinforcement Learning from Human Feedback",
        d: ["Random Learning from Humans", "Real Learning from Hardware", "Recursive Learning"],
        explanation: "RLHF aligns models to be helpful and safe (Module 14).",
        cluster: 4
      },
      {
        q: "Catastrophic forgetting happens when:",
        a: "New training overwrites old knowledge",
        d: ["Model gets too large", "Tokens are lost", "Safety filters fail"],
        explanation: "LoRA avoids this by only updating adapters (Module 05).",
        cluster: 4
      },
      {
        q: "Prompt injection is:",
        a: "Tricking the model with hidden instructions",
        d: ["Improving prompt quality", "Adding safety", "Fine-tuning"],
        explanation: "Hidden text can override system prompts (Module 07).",
        cluster: 4
      },
      {
        q: "Guardrails are:",
        a: "Safety filters to prevent harmful output",
        d: ["Speed optimizers", "Token counters", "Style transfer tools"],
        explanation: "Guardrails block unsafe responses (Module 07).",
        cluster: 4
      },
      {
        q: "Hallucinations are the #1 risk because:",
        a: "They sound confident but can be wrong",
        d: ["They are rare", "They only happen in small models", "They are easy to fix"],
        explanation: "Always verify AI outputs against sources (Module 07).",
        cluster: 4
      },
      // ... (15 more for cluster 4)

      // CLUSTER 5: Workflow & Governance (20)
      {
        q: "Human-in-the-loop means:",
        a: "A person reviews AI output before final use",
        d: ["AI works alone", "Only for training", "Only for research"],
        explanation: "Essential for high-stakes decisions (Module 15).",
        cluster: 5
      },
      {
        q: "Evaluation frameworks are needed because:",
        a: "LLMs can sound confident but be wrong",
        d: ["They are expensive", "Models are always accurate", "For marketing"],
        explanation: "Validate AI in your specific context (Module 15).",
        cluster: 5
      },
      {
        q: "Workflow orchestration refers to:",
        a: "Chaining multiple AI tools together",
        d: ["Single prompt only", "Manual processes", "Data storage"],
        explanation: "Agents orchestrate complex workflows (Module 15).",
        cluster: 5
      },
      {
        q: "Governance in medical AI includes:",
        a: "Policies, auditing, and validation",
        d: ["Optional for research", "Only for large hospitals", "No human oversight"],
        explanation: "Governance ensures responsible use (Module 15).",
        cluster: 5
      },
      {
        q: "Best practice for clinical use:",
        a: "Never trust AI output without human verification",
        d: ["Trust AI completely", "Use AI only for research", "Let AI make final decisions"],
        explanation: "AI is a tool, not a replacement (Module 07).",
        cluster: 5
      },
      // ... (15 more for cluster 5)
    ];

    // Shuffle and select 20 balanced questions
    function getRandomQuestions() {
      const selected = [];
      for (let c = 1; c <= 5; c++) {
        const clusterQs = questionBank.filter(q => q.cluster === c);
        const shuffled = [...clusterQs].sort(() => 0.5 - Math.random());
        selected.push(...shuffled.slice(0, 4));
      }
      return selected.sort(() => 0.5 - Math.random());
    }

    const questions = getRandomQuestions();
    const form = document.getElementById('fluency-quiz');

    questions.forEach((q, i) => {
      const allAnswers = [q.a, ...q.d];
      const shuffledAnswers = [...allAnswers].sort(() => 0.5 - Math.random());
      const correctIndex = shuffledAnswers.indexOf(q.a);

      const div = document.createElement('div');
      div.className = 'question-card bg-white rounded-2xl shadow-lg p-6 border border-slate-100';
      div.innerHTML = `
        <p class="text-sm md:text-base font-medium text-slate-800 mb-5">${i+1}. ${q.q}</p>
        <div class="space-y-3">
          ${shuffledAnswers.map((ans, j) => `
            <label class="flex items-start gap-4 cursor-pointer p-3 rounded-xl hover:bg-purple-50 transition">
              <input type="radio" name="q${i}" value="${j === correctIndex ? 'correct' : 'wrong'}" class="mt-1.5 w-5 h-5 text-purple-600">
              <span class="text-sm md:text-base">${ans}</span>
            </label>
          `).join('')}
        </div>
      `;
      form.appendChild(div);
    });

    form.innerHTML += `<div class="text-center mt-16"><button type="submit" class="px-16 py-6 bg-gradient-to-r from-purple-600 to-indigo-700 text-white text-xl font-bold rounded-full shadow-2xl hover:shadow-3xl transform hover:scale-105 transition-all duration-300">Generate My Fluency Profile →</button></div>`;

    form.addEventListener('submit', function(e) {
      e.preventDefault();
      const scores = [0,0,0,0,0];
      let wrongQuestions = [];

      questions.forEach((q, i) => {
        const selected = form.querySelector(`input[name="q${i}"]:checked`);
        if (selected && selected.value === "correct") {
          scores[q.cluster - 1]++;
        } else if (selected) {
          wrongQuestions.push({q: q.q, your: selected.nextElementSibling.textContent.trim(), correct: q.a, explanation: q.explanation});
        }
      });

      const percentages = scores.map(s => (s / 4) * 100);
      const avg = Math.round(percentages.reduce((a,b)=>a+b)/5);

      document.getElementById('result').classList.remove('hidden');
      document.getElementById('overall-score').textContent = `Overall Fluency: ${avg}%`;

      if (avg >= 90) confetti({particleCount: 200, spread: 70, origin: { y: 0.6 }});

      new Chart(document.getElementById('radarChart'), {
        type: 'radar',
        data: {
          labels: ["Prompt Mastery", "Model Fundamentals", "Practical Use Cases", "Safety & Evaluation", "Workflow & Governance"],
          datasets: [{
            label: 'Your Fluency',
            data: percentages,
            backgroundColor: 'rgba(139,92,246,0.2)',
            borderColor: '#8b5cf6',
            borderWidth: 5,
            pointBackgroundColor: '#7c3aed',
            pointRadius: 8
          }]
        },
        options: {
          scales: { r: { min: 0, max: 100, ticks: { stepSize: 20 } } },
          plugins: { legend: { display: false } }
        }
      });

      const weak = percentages.map((p,i) => p < 70 ? ["Prompt Mastery","Model Fundamentals","Practical Use Cases","Safety & Evaluation","Workflow & Governance"][i] : null).filter(Boolean);
      document.getElementById('personalized-path').innerHTML = weak.length
        ? `<p class="text-2xl"><strong>Focus areas:</strong> ${weak.join(', ')}</p><p class="text-xl mt-4">Watch the related MicroModules to level up!</p>`
        : "<p class='text-4xl text-green-600 font-bold'>Outstanding!</p><p class='text-2xl mt-4'>You have excellent fluency across all domains!</p>";

      // Review wrong answers
      const review = document.getElementById('review-section');
      if (wrongQuestions.length === 0) {
        review.innerHTML = "<p class='text-2xl text-green-600 font-bold text-center'>Perfect score! You got everything right!</p>";
      } else {
        review.innerHTML = `<p class="text-xl font-bold text-slate-700 mb-6">${wrongQuestions.length} question(s) to review:</p>` + wrongQuestions.map((w, i) => `
          <div class="bg-red-50 border-l-4 border-red-500 p-6 rounded-r-xl">
            <p class="font-medium text-slate-800">${w.q}</p>
            <p class="mt-3"><span class="text-red-700 font-semibold">You answered:</span> ${w.your}</p>
            <p class="mt-2"><span class="text-green-700 font-semibold">Correct answer:</span> ${w.correct}</p>
            <p class="mt-2 text-sm text-slate-600 italic">${w.explanation}</p>
          </div>
        `).join('');
      }

      window.scrollTo({ top: 0, behavior: 'smooth' });
    });
  </script>
</body>
</html>
